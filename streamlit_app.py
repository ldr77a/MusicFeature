import streamlit as st
import os
import json
import pandas as pd
import numpy as np
import librosa
import joblib
import warnings
from scipy.stats import skew, kurtosis
import tempfile
import time
import plotly.express as px
import plotly.graph_objects as go
from PIL import Image
import streamlit.components.v1 as components
import base64

# Librosa ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", message="n_fft=.*is too large for input signal of length.*")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ìŒì•… ê°ì • ë¶„ì„ê¸°",
    page_icon="ğŸµ",
    layout="wide",
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        text-align: center;
        color: #1f77b4;
        font-size: 3rem;
        margin-bottom: 2rem;
    }
    .emotion-card {
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        border-radius: 15px;
        padding: 1.5rem;
        text-align: center;
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
    }
    .segment-item {
        padding: 0.5rem;
        margin: 0.25rem 0;
        border-radius: 8px;
        background: #f8f9fa;
        border-left: 4px solid #007bff;
    }
    .confidence-high { border-left-color: #28a745; }
    .confidence-medium { border-left-color: #ffc107; }
    .confidence-low { border-left-color: #dc3545; }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_emotion_images():
    """ê°ì • ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    emotion_images = {}
    emotion_files = {
        'Sad': 'Sad_Emotion.png',
        'Annoying': 'Annoying_Emotion.png', 
        'Calme': 'Clame_Emotion.png',
        'Amusing': 'Amusing_Emotion.png'
    }
    
    for emotion, filename in emotion_files.items():
        img_path = os.path.join('Emotion_png', filename)
        if os.path.exists(img_path):
            emotion_images[emotion] = Image.open(img_path)
    
    return emotion_images

def image_to_base64(image):
    """PIL ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
    import io
    buffer = io.BytesIO()
    image.save(buffer, format='PNG')
    img_str = base64.b64encode(buffer.getvalue()).decode()
    return f"data:image/png;base64,{img_str}"

def create_rhythm_game_html(results_df, emotion_images, audio_file_bytes):
    """ë¦¬ë“¬ê²Œì„ HTML ìƒì„± (íƒ€ì´ë° ê°œì„  ìµœì¢… ë²„ì „)"""
    
    audio_b64 = base64.b64encode(audio_file_bytes).decode('utf-8')
    emotion_images_b64 = {emotion: image_to_base64(img) for emotion, img in emotion_images.items()}
    results_json = results_df.to_dict('records')
    
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <style>
            /* CSSëŠ” ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ */
            .game-container {{ position: relative; width: 100%; height: 500px; background: #f0f2f6; border-radius: 15px; overflow: hidden; box-shadow: 0 10px 30px rgba(0,0,0,0.1); }}
            .audio-player {{ position: absolute; bottom: 10px; left: 50%; transform: translateX(-50%); width: 90%; z-index: 10; }}
            .audio-player audio {{ width: 100%; }}
            .floating-note {{ position: absolute; width: 80px; height: 80px; animation: noteAnimation 1.5s ease-out forwards; pointer-events: none; z-index: 50; }}
            .floating-note img {{ width: 100%; height: 100%; border-radius: 50%; box-shadow: 0 5px 15px rgba(0,0,0,0.2); }}
            @keyframes noteAnimation {{
                0% {{ opacity: 0; transform: scale(0.5); }}
                20% {{ opacity: 1; transform: scale(1.1); }}
                80% {{ opacity: 1; transform: scale(0.9); }}
                100% {{ opacity: 0; transform: scale(0.5); }}
            }}
        </style>
    </head>
    <body>
        <div class="game-container" id="gameContainer">
            <div class="audio-player">
                <audio id="audioPlayer" controls src="data:audio/mp3;base64,{audio_b64}"></audio>
            </div>
        </div>
        
        <script>
            const notes = {json.dumps(results_json)};
            const emotionImages = {json.dumps(emotion_images_b64)};
            
            const gameContainer = document.getElementById('gameContainer');
            const audioPlayer = document.getElementById('audioPlayer');

            // ==========================================================
            // ===            ìµœì í™”ëœ ë…¸íŠ¸ íƒ€ì´ë° ì‹œìŠ¤í…œ              ===
            // ==========================================================

            // 1. ë” ë¹ ë¥¸ ë°˜ì‘ì„ ìœ„í•œ ì§§ì€ Lookahead (0.05ì´ˆ = 50ms)
            const lookahead = 0.05; 
            
            // ì²˜ë¦¬í•´ì•¼ í•  ë‹¤ìŒ ë…¸íŠ¸ì˜ ì¸ë±ìŠ¤
            let nextNoteIndex = 0;

            // ë…¸íŠ¸ ìƒì„± í•¨ìˆ˜ (ì¦‰ì‹œ ìƒì„±ìœ¼ë¡œ ê°œì„ )
            function spawnEmotionNote(emotion) {{
                if (!emotionImages[emotion]) return;
                const note = document.createElement('div');
                note.className = 'floating-note';
                note.style.left = `${{Math.random() * (gameContainer.offsetWidth - 90) + 5}}px`;
                note.style.top = `${{Math.random() * (gameContainer.offsetHeight - 150) + 5}}px`;
                note.innerHTML = `<img src="${{emotionImages[emotion]}}" alt="${{emotion}}">`;
                gameContainer.appendChild(note);
                setTimeout(() => note.remove(), 1500);
            }}

            // 2. ê³ ì£¼íŒŒ íƒ€ì´ë° ì²´í¬ (ë” ì •í™•í•œ ë™ê¸°í™”)
            function gameLoop() {{
                if (!audioPlayer.paused) {{
                    const currentTime = audioPlayer.currentTime;

                    // ë” ì ê·¹ì ì¸ ë…¸íŠ¸ ìƒì„± (lookahead ë²”ìœ„ ë‚´ ëª¨ë“  ë…¸íŠ¸ ì¦‰ì‹œ ì²˜ë¦¬)
                    while (nextNoteIndex < notes.length && notes[nextNoteIndex].beat_time <= currentTime + lookahead) {{
                        
                        const note = notes[nextNoteIndex];
                        const timeDiff = note.beat_time - currentTime;
                        
                        // ì‹œê°„ ì°¨ì´ê°€ ë§¤ìš° ì‘ìœ¼ë©´ ì¦‰ì‹œ ìƒì„±, ì•„ë‹ˆë©´ ì •í™•í•œ íƒ€ì´ë°ì— ì˜ˆì•½
                        if (timeDiff <= 0.02) {{  // 20ms ì´ë‚´ë©´ ì¦‰ì‹œ ìƒì„±
                            spawnEmotionNote(note.emotion);
                            console.log(`âš¡ ${{note.beat_time.toFixed(2)}}s: ${{note.emotion}} ë…¸íŠ¸ ì¦‰ì‹œ ìƒì„±!`);
                        }} else {{
                            const delay = timeDiff * 1000;
                            setTimeout(() => {{
                                spawnEmotionNote(note.emotion);
                                console.log(`ğŸµ ${{note.beat_time.toFixed(2)}}s: ${{note.emotion}} ë…¸íŠ¸ ìƒì„±! (ì§€ì—°: ${{delay.toFixed(0)}}ms)`);
                            }}, delay);
                        }}
                        
                        nextNoteIndex++;
                    }}
                }}
                
                // ë” ë†’ì€ ì£¼íŒŒìˆ˜ë¡œ ì²´í¬ (60fps ëŒ€ì‹  120fps ìˆ˜ì¤€)
                setTimeout(() => requestAnimationFrame(gameLoop), 8);
            }}

            // ì‹œê°„ ì´ë™ ì‹œ ì¸ë±ìŠ¤ ì¬ì„¤ì • (ê°œì„ ëœ ë²„ì „)
            audioPlayer.addEventListener('seeked', () => {{
                const currentTime = audioPlayer.currentTime;
                nextNoteIndex = 0;
                
                // í˜„ì¬ ì‹œê°„ ì´í›„ì˜ ì²« ë²ˆì§¸ ë…¸íŠ¸ ì°¾ê¸°
                for (let i = 0; i < notes.length; i++) {{
                    if (notes[i].beat_time >= currentTime - 0.1) {{  // 0.1ì´ˆ ì—¬ìœ 
                        nextNoteIndex = i;
                        break;
                    }}
                }}
                console.log(`â­ï¸ ì‹œê°„ ì´ë™ (${{currentTime.toFixed(2)}}s). ë‹¤ìŒ ë…¸íŠ¸ ì¸ë±ìŠ¤: ${{nextNoteIndex}}`);
            }});

            // ==========================================================
            // ===              ìµœì í™”ëœ íƒ€ì´ë° ì‹œìŠ¤í…œ ë              ===
            // ==========================================================

            console.log('ë¦¬ë“¬ê²Œì„ ì´ˆê¸°í™” ì™„ë£Œ. ë…¸íŠ¸ ê°œìˆ˜:', notes.length);
            
            // ê²Œì„ ë£¨í”„ ì‹œì‘
            gameLoop();
        </script>
    </body>
    </html>
    """
    
    return html_content

def extract_features(y, sr, start_time, end_time):
    """
    ì£¼ì–´ì§„ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ìŒì•… íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # ì‹œê°„ ì¸ë±ìŠ¤ë¥¼ ìƒ˜í”Œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    start_idx = int(start_time * sr)
    end_idx = int(end_time * sr)
    
    # ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
    y_segment = y[start_idx:end_idx]
    
    # ì‹ í˜¸ì˜ ê¸¸ì´ì— ë§ê²Œ ì ì ˆí•œ n_fft ê°’ ì„¤ì •
    signal_length = len(y_segment)
    
    if signal_length < 1024:
        n_fft = 2 ** int(np.log2(signal_length))
        n_fft = max(32, n_fft)
        hop_length = n_fft // 4
    else:
        n_fft = 1024
        hop_length = 256
    
    # íŠ¹ì§•ë“¤ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    features = {}
    
    # 1. ë‹¤ì–‘í•œ Chroma íŠ¹ì§•ë“¤
    chroma_cens = librosa.feature.chroma_cens(y=y_segment, sr=sr, hop_length=hop_length)
    chroma_stft = librosa.feature.chroma_stft(y=y_segment, sr=sr, n_fft=n_fft, hop_length=hop_length)
    chroma_cqt = librosa.feature.chroma_cqt(y=y_segment, sr=sr, hop_length=hop_length)
    
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    
    for i, note in enumerate(note_names):
        features[f'{note}_CENS_mean'] = np.mean(chroma_cens[i])
        features[f'{note}_CENS_std'] = np.std(chroma_cens[i])
        features[f'{note}_STFT_mean'] = np.mean(chroma_stft[i])
        features[f'{note}_STFT_std'] = np.std(chroma_stft[i])
        features[f'{note}_CQT_mean'] = np.mean(chroma_cqt[i])
        features[f'{note}_CQT_std'] = np.std(chroma_cqt[i])
    
    # 2. í™•ì¥ëœ MFCC
    n_mfcc = 20
    mfcc = librosa.feature.mfcc(y=y_segment, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)
    mfcc_delta = librosa.feature.delta(mfcc)
    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)
    
    for i in range(n_mfcc):
        features[f'MFCC_{i}_mean'] = np.mean(mfcc[i])
        features[f'MFCC_{i}_std'] = np.std(mfcc[i])
        
        if i < 10:
            features[f'MFCC_{i}_kurtosis'] = kurtosis(mfcc[i], fisher=True, nan_policy='omit')
            features[f'MFCC_{i}_skewness'] = skew(mfcc[i], nan_policy='omit')
        
        features[f'MFCC_delta_{i}_mean'] = np.mean(mfcc_delta[i])
        features[f'MFCC_delta_{i}_std'] = np.std(mfcc_delta[i])
        
        if i < 10:
            features[f'MFCC_delta2_{i}_mean'] = np.mean(mfcc_delta2[i])
            features[f'MFCC_delta2_{i}_std'] = np.std(mfcc_delta2[i])
    
    # 3. Tonnetz
    y_harmonic = librosa.effects.harmonic(y_segment)
    tonnetz = librosa.feature.tonnetz(y=y_harmonic, sr=sr)
    
    tonnetz_names = ['tonnetz_0', 'tonnetz_1', 'tonnetz_2', 'tonnetz_3', 'tonnetz_4', 'tonnetz_5']
    for i, name in enumerate(tonnetz_names):
        features[f'{name}_mean'] = np.mean(tonnetz[i])
        features[f'{name}_std'] = np.std(tonnetz[i])
    
    # 4. ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±ë“¤
    spectral_centroid = librosa.feature.spectral_centroid(y=y_segment, sr=sr, n_fft=n_fft, hop_length=hop_length)
    features['spectral_centroid_mean'] = np.mean(spectral_centroid)
    features['spectral_centroid_std'] = np.std(spectral_centroid)
    
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y_segment, sr=sr, n_fft=n_fft, hop_length=hop_length)
    features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)
    features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)
    
    spectral_contrast = librosa.feature.spectral_contrast(y=y_segment, sr=sr, n_fft=n_fft, hop_length=hop_length)
    for i in range(7):
        features[f'contrast_{i}_mean'] = np.mean(spectral_contrast[i])
        features[f'contrast_{i}_std'] = np.std(spectral_contrast[i])
    
    spectral_flatness = librosa.feature.spectral_flatness(y=y_segment, n_fft=n_fft, hop_length=hop_length)
    features['spectral_flatness_mean'] = np.mean(spectral_flatness)
    features['spectral_flatness_std'] = np.std(spectral_flatness)
    
    spectral_rolloff = librosa.feature.spectral_rolloff(y=y_segment, sr=sr, n_fft=n_fft, hop_length=hop_length)
    features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)
    features['spectral_rolloff_std'] = np.std(spectral_rolloff)
    
    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y_segment, hop_length=hop_length)
    features['zero_crossing_rate_mean'] = np.mean(zero_crossing_rate)
    features['zero_crossing_rate_std'] = np.std(zero_crossing_rate)
    
    # 5. ë¦¬ë“¬ íŠ¹ì„±ë“¤
    tempo, _ = librosa.beat.beat_track(y=y_segment, sr=sr, hop_length=hop_length)
    if hasattr(tempo, 'item'):
        tempo = tempo.item()
    else:
        tempo = float(tempo)
    features['tempo'] = tempo
    
    y_perc = librosa.effects.percussive(y_segment)
    onset_env = librosa.onset.onset_strength(y=y_perc, sr=sr, hop_length=hop_length)
    ac = librosa.autocorrelate(onset_env, max_size=sr // hop_length // 2)
    ac = librosa.util.normalize(ac, norm=np.inf)
    
    peaks = librosa.util.peak_pick(ac, pre_max=10, post_max=10, pre_avg=10, post_avg=10, delta=0.5, wait=10)
    
    if len(peaks) > 0:
        features['rhythm_periodicity'] = len(peaks)
        features['rhythm_strength'] = np.mean(ac[peaks])
    else:
        features['rhythm_periodicity'] = 0
        features['rhythm_strength'] = 0
    
    features['onset_strength_mean'] = np.mean(onset_env)
    features['onset_strength_std'] = np.std(onset_env)
    
    # 6. Band Energy Ratio
    D = np.abs(librosa.stft(y_segment, n_fft=n_fft, hop_length=hop_length))
    
    n_bands = 3
    band_size = D.shape[0] // n_bands
    
    band_energy = []
    for i in range(n_bands):
        start_bin = i * band_size
        end_bin = min((i + 1) * band_size, D.shape[0])
        band_energy.append(np.sum(D[start_bin:end_bin, :] ** 2))
    
    total_energy = np.sum(band_energy)
    if total_energy > 0:
        for i in range(n_bands):
            features[f'band_energy_ratio_{i}'] = band_energy[i] / total_energy
    else:
        for i in range(n_bands):
            features[f'band_energy_ratio_{i}'] = 0
    
    # 7. RMS ì—ë„ˆì§€
    rms = librosa.feature.rms(y=y_segment, hop_length=hop_length)
    features['rms_mean'] = np.mean(rms)
    features['rms_std'] = np.std(rms)
    
    # 8. í•˜ëª¨ë‹‰/í¼ì»¤ì‹œë¸Œ ì„±ë¶„ ë¹„ìœ¨
    y_harmonic = librosa.effects.harmonic(y_segment)
    y_percussive = librosa.effects.percussive(y_segment)
    
    harmonic_energy = np.sum(y_harmonic ** 2)
    percussive_energy = np.sum(y_percussive ** 2)
    total_energy = harmonic_energy + percussive_energy
    
    if total_energy > 0:
        features['harmonic_ratio'] = harmonic_energy / total_energy
        features['percussive_ratio'] = percussive_energy / total_energy
    else:
        features['harmonic_ratio'] = 0
        features['percussive_ratio'] = 0
    
    features['start_time'] = start_time
    features['end_time'] = end_time
    
    # NaN ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´
    for key in features:
        if np.isnan(features[key]):
            features[key] = 0
    
    return features

def detect_rhythmic_onsets_balanced(file_path, target_note_count=100, chunk_duration=3.0, min_interval=0.3):
    """
    ğŸ® ë¦¬ë“¬ê²Œì„ ìµœì í™”: HPSS + ì‹œê°„ ê· ë“± ë¶„ë°° + ê°„ê²© ì¡°ì ˆ
    
    1. HPSSë¡œ ê¹¨ë—í•œ ë“œëŸ¼/íƒ€ì•…ê¸° íŒŒíŠ¸ë§Œ ì¶”ì¶œ
    2. ê³¡ì„ ì¼ì • êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê° êµ¬ê°„ì—ì„œ ê· ë“±í•˜ê²Œ ë…¸íŠ¸ ì„ íƒ
    3. ìµœì†Œ ê°„ê²© í•„í„°ë§ìœ¼ë¡œ ì—°ë‹¬ì•„ ë‚˜ì˜¤ëŠ” ë…¸íŠ¸ ë°©ì§€
    4. ë¦¬ë“¬ê²Œì„ë‹¤ìš´ ìì—°ìŠ¤ëŸ¬ìš´ ë…¸íŠ¸ ë¶„í¬ ë‹¬ì„±
    """
    try:
        y, sr = librosa.load(file_path)
        duration = librosa.get_duration(y=y, sr=sr)

        # 1. HPSSë¡œ ë¦¬ë“¬ íŒŒíŠ¸ë§Œ ë¶„ë¦¬ (í•µì‹¬ ê°œì„ )
        y_harmonic, y_percussive = librosa.effects.hpss(y)

        # 2. ë¦¬ë“¬ íŒŒíŠ¸ì—ì„œë§Œ Onset Strength ê³„ì‚°
        onset_env = librosa.onset.onset_strength(y=y_percussive, sr=sr)
        
        # 3. ëª¨ë“  ë¦¬ë“¬ Onset ê°ì§€
        all_onset_frames = librosa.onset.onset_detect(
            onset_envelope=onset_env, 
            sr=sr, 
            units='frames', 
            backtrack=True
        )
        
        if len(all_onset_frames) == 0:
            st.warning("ë¦¬ë“¬ ì˜¨ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # 4. ì‹œê°„ êµ¬ê°„ë³„ ê· ë“± ë¶„ë°° ì‹œìŠ¤í…œ
        total_chunks = int(np.ceil(duration / chunk_duration))
        notes_per_chunk = max(1, target_note_count // total_chunks)
        
        st.info(f"ğŸµ ê³¡ ê¸¸ì´: {duration:.1f}ì´ˆ â†’ {total_chunks}ê°œ êµ¬ê°„ìœ¼ë¡œ ë¶„í• ")
        st.info(f"ğŸ¯ êµ¬ê°„ë‹¹ ëª©í‘œ ë…¸íŠ¸: {notes_per_chunk}ê°œ (ì´ ëª©í‘œ: {target_note_count}ê°œ)")

        final_onset_frames = []
        actual_notes_selected = 0
        
        # 5. ê° ì‹œê°„ êµ¬ê°„ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ìµœê°• ë…¸íŠ¸ ì„ íƒ
        for chunk_idx in range(total_chunks):
            start_time = chunk_idx * chunk_duration
            end_time = min((chunk_idx + 1) * chunk_duration, duration)
            
            # í˜„ì¬ êµ¬ê°„ì˜ í”„ë ˆì„ ë²”ìœ„
            start_frame = librosa.time_to_frames(start_time, sr=sr)
            end_frame = librosa.time_to_frames(end_time, sr=sr)
            
            # ì´ êµ¬ê°„ì— ì†í•˜ëŠ” ì˜¨ì…‹ë“¤ë§Œ í•„í„°ë§
            chunk_onsets = []
            for frame in all_onset_frames:
                if start_frame <= frame < end_frame:
                    chunk_onsets.append({
                        'frame': frame,
                        'strength': onset_env[frame],
                        'time': librosa.frames_to_time(frame, sr=sr)
                    })
            
            # êµ¬ê°„ ë‚´ ì˜¨ì…‹ì´ ìˆë‹¤ë©´ ê°•ë„ìˆœ ì •ë ¬ í›„ ìƒìœ„ ì„ íƒ
            if chunk_onsets:
                # ê°•ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                chunk_onsets.sort(key=lambda x: x['strength'], reverse=True)
                
                # ì´ êµ¬ê°„ì—ì„œ ì„ íƒí•  ë…¸íŠ¸ ìˆ˜ ê²°ì •
                available_notes = len(chunk_onsets)
                select_count = min(notes_per_chunk, available_notes)
                
                # ë§ˆì§€ë§‰ êµ¬ê°„ì—ì„œëŠ” ë‚¨ì€ ë…¸íŠ¸ í• ë‹¹ëŸ‰ ëª¨ë‘ ì‚¬ìš©
                if chunk_idx == total_chunks - 1:
                    remaining_quota = target_note_count - actual_notes_selected
                    select_count = min(remaining_quota, available_notes)
                
                # ì„ íƒëœ ì˜¨ì…‹ë“¤ì˜ í”„ë ˆì„ ì¶”ê°€
                selected_onsets = chunk_onsets[:select_count]
                for onset in selected_onsets:
                    final_onset_frames.append(onset['frame'])
                
                actual_notes_selected += select_count
                
                # ë””ë²„ê¹… ì •ë³´
                chunk_time_range = f"{start_time:.1f}~{end_time:.1f}ì´ˆ"
                #st.write(f"ğŸ“ êµ¬ê°„ {chunk_idx+1} ({chunk_time_range}): {available_notes}ê°œ ì¤‘ {select_count}ê°œ ì„ íƒ")

                 # 6. ì‹œê°„ìˆœ ì •ë ¬
        final_onset_frames.sort()
        
        # 7. í”„ë ˆì„ì„ ì‹œê°„ìœ¼ë¡œ ë³€í™˜
        onset_times_raw = librosa.frames_to_time(np.array(final_onset_frames), sr=sr)
        
        # 8. ìµœì†Œ ê°„ê²© í•„í„°ë§ (ì—°ë‹¬ì•„ ë‚˜ì˜¤ëŠ” ë…¸íŠ¸ ë°©ì§€)
        filtered_times = []
        last_time = -999  # ì¶©ë¶„íˆ ì‘ì€ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
        
        for time in onset_times_raw:
            if time - last_time >= min_interval:
                filtered_times.append(time)
                last_time = time
        
        # 9. ìµœì¢… ê²°ê³¼ ë³´ê³ 
        removed_count = len(onset_times_raw) - len(filtered_times)
        st.success(f" ë¦¬ë“¬ê²Œì„ ë…¸íŠ¸ ìƒì„± ì™„ë£Œ!")
        st.info(f" ìµœì¢… ë…¸íŠ¸: {len(filtered_times)}ê°œ (ëª©í‘œ: {target_note_count}ê°œ)")
        if removed_count > 0:
            st.info(f" ê°„ê²© ì¡°ì ˆ: {removed_count}ê°œ ë…¸íŠ¸ ì œê±° (ìµœì†Œ ê°„ê²©: {min_interval}ì´ˆ)")
        st.info(f" í‰ê·  ë…¸íŠ¸ ê°„ê²©: {duration/len(filtered_times):.2f}ì´ˆ")
        
        return filtered_times

    except Exception as e:
        st.error(f"ë¦¬ë“¬ ì˜¨ì…‹ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return []

@st.cache_resource
def load_trained_model_and_scaler(model_dir="Model"):
    """
    í•™ìŠµëœ SMOTE ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    model_path = os.path.join(model_dir, 'smote_model2.pkl')
    scaler_path = os.path.join(model_dir, 'smote_scaler2.pkl')
    features_path = os.path.join(model_dir, 'smote_features.txt')
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
    if not os.path.exists(scaler_path):
        raise FileNotFoundError(f"ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {scaler_path}")
    if not os.path.exists(features_path):
        raise FileNotFoundError(f"íŠ¹ì§• ëª©ë¡ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {features_path}")
    
    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
    
    with open(features_path, 'r') as f:
        feature_names = [line.strip() for line in f.readlines()]
    
    emotion_mapping = {
        1: 'Sad',
        2: 'Annoying', 
        3: 'Calme',
        4: 'Amusing'
    }
    
    return model, scaler, feature_names, emotion_mapping

def apply_emotion_threshold(probabilities, thresholds):
    """
    ê°ì •ë³„ í™•ë¥  ì„ê³„ê°’ì„ ì ìš©í•˜ì—¬ ë” ê· í˜•ì¡íŒ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    original_emotion_id = np.argmax(probabilities) + 1
    
    adjusted_probs = probabilities.copy()
    
    if 4 in thresholds:
        if probabilities[3] < thresholds[4]:
            adjusted_probs[3] *= 0.55
    
    for emotion_id, threshold in thresholds.items():
        if emotion_id <= 3:
            if probabilities[emotion_id-1] >= threshold:
                adjusted_probs[emotion_id-1] *= 1.2
    
    adjusted_emotion_id = np.argmax(adjusted_probs) + 1
    confidence = np.max(adjusted_probs)
    
    return adjusted_emotion_id, confidence, original_emotion_id

def process_audio_on_beats(file_path, beat_times, model, scaler, feature_names, emotion_mapping, segment_duration=2.0):
    """
    MP3 íŒŒì¼ì„ 'ë¹„íŠ¸ ì‹œì 'ì„ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ê°ì • ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    try:
        y, sr = librosa.load(file_path, sr=None)
    except Exception as e:
        raise ValueError(f"ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    duration = librosa.get_duration(y=y, sr=sr)
    
    emotion_thresholds = {
        1: 0.4,  # Sad
        2: 0.4,  # Annoying  
        3: 0.4,  # Calme
        4: 0.6   # Amusing
    }
    
    results = []
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_beats = len(beat_times)
    
    # ë³€ê²½ì : ê³ ì •ëœ ì‹œê°„ ëŒ€ì‹  beat_times ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ë¶„ì„
    for i, beat_time in enumerate(beat_times):
        # ê° ë¹„íŠ¸ ì‹œì ì„ ì¤‘ì‹¬ìœ¼ë¡œ segment_duration(2ì´ˆ) ë§Œí¼ì˜ ë¶„ì„ì°½ ì„¤ì •
        start_time = max(0, beat_time - (segment_duration / 2))
        end_time = min(duration, beat_time + (segment_duration / 2))

        # ë¶„ì„ì°½ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆëœ€
        if end_time - start_time < 0.5:
            continue
            
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        progress = (i + 1) / total_beats
        progress_bar.progress(progress)
        status_text.text(f'ê°ì • ë¶„ì„ ì¤‘... {i+1}/{total_beats} ë¹„íŠ¸')

        try:
            features = extract_features(y, sr, start_time, end_time)
        except Exception as e:
            st.warning(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨ (ë¹„íŠ¸: {beat_time:.2f}s): {e}")
            continue
        
        selected_features = [features.get(name, 0) for name in feature_names]
        X = np.array(selected_features).reshape(1, -1)
        
        try:
            X_scaled = scaler.transform(X)
            emotion_prob = model.predict_proba(X_scaled)[0]
            
            emotion_id, confidence, _ = apply_emotion_threshold(emotion_prob, emotion_thresholds)
            emotion_name = emotion_mapping.get(emotion_id, 'Unknown')
            
        except Exception as e:
            st.warning(f"ê°ì • ì˜ˆì¸¡ ì‹¤íŒ¨ (ë¹„íŠ¸: {beat_time:.2f}s): {e}")
            continue
        
        result = {
            'beat_id': i + 1,
            'beat_time': beat_time,  # ì¤‘ìš”: ë…¸íŠ¸ê°€ ë‚˜íƒ€ë‚  ì •í™•í•œ ì‹œê°„
            'emotion': emotion_name,
            'confidence': confidence,
            'analysis_start': start_time, # ì°¸ê³ ìš©: ë¶„ì„ì— ì‚¬ìš©ëœ ì˜¤ë””ì˜¤ ì‹œì‘
            'analysis_end': end_time,   # ì°¸ê³ ìš©: ë¶„ì„ì— ì‚¬ìš©ëœ ì˜¤ë””ì˜¤ ë
        }
        
        # ê° ê°ì •ì˜ í™•ë¥  ì¶”ê°€
        for idx, class_label in enumerate(model.classes_):
            prob_emotion_name = emotion_mapping.get(class_label, f'Unknown_{class_label}')
            result[f'prob_{prob_emotion_name}'] = emotion_prob[idx]
        
        results.append(result)
        
    # ì§„í–‰ ìƒí™© ì™„ë£Œ
    progress_bar.empty()
    status_text.empty()
    
    df = pd.DataFrame(results)
    return df

def main():
    st.markdown('<h1 class="main-header">ğŸµ ìŒì•… ê°ì • ë¶„ì„ê¸°</h1>', unsafe_allow_html=True)
    
    # ë¦¬ë“¬ê²Œì„ ìµœì í™” ì„¤ì •
    with st.sidebar:
        st.header("ğŸ® ë¦¬ë“¬ê²Œì„ ì„¤ì •")
        
        target_notes = st.slider(
            "ì´ ë…¸íŠ¸ ê°œìˆ˜",
            min_value=50,
            max_value=400,
            value=120,
            step=10,
            help="ì „ì²´ ê³¡ì—ì„œ ìƒì„±í•  ì´ ë…¸íŠ¸ ê°œìˆ˜. ë¦¬ë“¬ê²Œì„ì˜ ì „ì²´ì ì¸ ë‚œì´ë„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."
        )
        
        chunk_duration = st.slider(
            "ë¶„ë°° êµ¬ê°„ ê¸¸ì´ (ì´ˆ)",
            min_value=2.0,
            max_value=5.0,
            value=3.0,
            step=0.5,
            help="ê³¡ì„ ì´ ê¸¸ì´ë¡œ ë‚˜ëˆ„ì–´ ê° êµ¬ê°„ì—ì„œ ê· ë“±í•˜ê²Œ ë…¸íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì§§ì„ìˆ˜ë¡ ë” ê· ë“±í•œ ë¶„ë°°ê°€ ë©ë‹ˆë‹¤."
        )
        
        min_interval = st.slider(
            "ìµœì†Œ ë…¸íŠ¸ ê°„ê²© (ì´ˆ)",
            min_value=0.1,
            max_value=0.8,
            value=0.3,
            step=0.1,
            help="ì—°ë‹¬ì•„ ë‚˜ì˜¤ëŠ” ë…¸íŠ¸ ì‚¬ì´ì˜ ìµœì†Œ ê°„ê²©ì…ë‹ˆë‹¤. í´ìˆ˜ë¡ ë…¸íŠ¸ê°€ ë” ì—¬ìœ ë¡­ê²Œ ë°°ì¹˜ë©ë‹ˆë‹¤."
        )
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "MP3 íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
        type=['mp3'],
        help="ìµœëŒ€ 50MBê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
    )
    
    if uploaded_file is not None:
        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        file_details = {
            "íŒŒì¼ëª…": uploaded_file.name,
            "íŒŒì¼ í¬ê¸°": f"{uploaded_file.size / (1024*1024):.2f} MB"
        }
        
        with st.expander(" íŒŒì¼ ì •ë³´", expanded=True):
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"**íŒŒì¼ëª…:** {file_details['íŒŒì¼ëª…']}")
            with col2:
                st.write(f"**í¬ê¸°:** {file_details['íŒŒì¼ í¬ê¸°']}")
        
        # ìŒì•… í”Œë ˆì´ì–´ (ë¶„ì„ ì „ ë¯¸ë¦¬ë³´ê¸°)
        st.audio(uploaded_file, format='audio/mp3')
        
        # ë¶„ì„ ì‹œì‘ ë²„íŠ¼
        if st.button(" ê°ì • ë¶„ì„ ì‹œì‘", type="primary"):
            try:
                # íŒŒì¼ ë‚´ìš©ì„ ë¨¼ì € ì½ì–´ì„œ ì €ì¥
                audio_file_bytes = uploaded_file.read()
                
                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmp_file:
                    tmp_file.write(audio_file_bytes)
                    tmp_file_path = tmp_file.name
                
                with st.spinner("ğŸ® ë¦¬ë“¬ê²Œì„ ìµœì í™” ë¶„ì„ ì¤‘..."):
                    model, scaler, feature_names, emotion_mapping = load_trained_model_and_scaler()
                    
                    # ê· ë“± ë¶„ë°° ë¦¬ë“¬ ë¶„ì„ í˜¸ì¶œ
                    note_times = detect_rhythmic_onsets_balanced(
                        tmp_file_path, 
                        target_note_count=target_notes,
                        chunk_duration=chunk_duration,
                        min_interval=min_interval
                    )

                if not note_times:
                    st.error("ì´ ìŒì•…ì—ì„œ ì—°ì£¼ ë…¸íŠ¸ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # st.info ë©”ì‹œì§€ëŠ” detect_onsets í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬
                    
                    # ë³€ê²½ì  3: ìƒˆë¡œìš´ ê¸°ë°˜ ë¶„ì„ í•¨ìˆ˜ì— note_times ì „ë‹¬
                    results_df = process_audio_on_beats(
                        tmp_file_path, note_times, model, scaler, feature_names, emotion_mapping
                    )
                    
                    if not results_df.empty:
                        # display_resultsì—ëŠ” ì´ì œ strong_beatsë¥¼ ë„˜ê¸¸ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
                        display_results(results_df, uploaded_file.name, audio_file_bytes)
                    else:
                        st.error("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(tmp_file_path)
                
            except Exception as e:
                st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

def display_results(results_df, filename, audio_file_bytes=None):
    """ë¶„ì„ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    # ì´ì œ results_dfì— ëª¨ë“  ì •ë³´ê°€ ìˆìœ¼ë¯€ë¡œ, ì´ ë¶„ì„ ì„¸ê·¸ë¨¼íŠ¸ ëŒ€ì‹  ì´ ë…¸íŠ¸ ìˆ˜ë¥¼ í‘œì‹œ
    st.success(f" ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ {len(results_df)}ê°œì˜ ê°ì • ë…¸íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    
    # ê°ì • ì´ë¯¸ì§€ ë¡œë“œ
    emotion_images = load_emotion_images()
    
    if emotion_images and audio_file_bytes:
        st.header("ğŸ® ë¦¬ë“¬ê²Œì„ - ê°ì • ë…¸íŠ¸")
        
        try:
            # ë³€ê²½ì : strong_beats ì¸ì ì—†ì´ í˜¸ì¶œ
            rhythm_game_html = create_rhythm_game_html(results_df, emotion_images, audio_file_bytes)
            components.html(rhythm_game_html, height=600)
        except Exception as e:
            st.error(f"ë¦¬ë“¬ê²Œì„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            st.info("ì¼ë°˜ ìŒì•… í”Œë ˆì´ì–´ë¥¼ ëŒ€ì‹  í‘œì‹œí•©ë‹ˆë‹¤.")
            st.audio(audio_file_bytes, format='audio/mp3')
    else:
        st.header("ğŸµ ìŒì•… í”Œë ˆì´ì–´")
        if audio_file_bytes:
            st.audio(audio_file_bytes, format='audio/mp3')
        else:
            st.warning("ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
if __name__ == "__main__":
    main() 